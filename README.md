# ollama-serving

## ğŸ‘¥ Collaborators

<div align="center">

|                                                    íŒ€ì›                                                    |  ì´ë¦„  |
| :--------------------------------------------------------------------------------------------------------: | :----: |
|     <a href="https://github.com/gsgh3016"><img src="https://github.com/gsgh3016.png" width="100"></a>      | ê°•ê°ì°¬ |
| <a href="https://github.com/juyoung-song"> <img src="https://github.com/juyoung-song.png" width="100"></a> | ì†¡ì£¼ì˜ |

</div>

## Project Setup Guide

### 1. Download Ollama

[Ollama Github Repo ğŸ”—](https://github.com/ollama/ollama?tab=readme-ov-file)

### 2. Git Clone

```shell
$ git clone https://github.com/5hyun-AI/ollama-serving.git
$ cd ollama-serving
```

### 3. Create Virtual Environment

```shell
$ python -m venv .venv
$ source .venv/bin/activate # macOS
$ .\venv\Scripts\activate # windows
(.venv) $
```

### 4. Install Packages

```shell
(.venv) $ pip install -r requirements.txt
```

### 5. Set Ollama Server

- ë°±ê·¸ë¼ìš´ë“œì—ì„œ Ollama ì„œë²„ ì‹¤í–‰

```shell
(.venv) $ ollama serve &
# Ctrl + cë¡œ ë¹ ì ¸ë‚˜ì˜¤ê¸°
```

- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ(ì•½ 4.5GB)

```shell
(.venv) $ ollama run llama3:8b
```

### 6. execute FastAPI server

```shell
(.venv) $ uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 7. request POST in Postman

![image](src/image.png)

### 8. Ollama ë° FastAPI ì„œë²„ ì¢…ë£Œ

```shell
# FastAPIì¸ ê²½ìš°ì—ëŠ” Ctrl + c
(.venv) $ lsof -t -i:11434 | xargs kill -9
```
